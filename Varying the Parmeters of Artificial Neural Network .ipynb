{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cc2d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ce08ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "# Set TensorFlow random seed\n",
    "tensorflow.random.set_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "274ca4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "climVar1 = pd.read_csv(r\"C:\\Users\\rajim\\Downloads\\d set\\met_var_UK.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe0136ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_year</th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minTemp</th>\n",
       "      <th>meanTemp</th>\n",
       "      <th>vapPress</th>\n",
       "      <th>cldCover</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/1901</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/1901</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>81.2</td>\n",
       "      <td>59.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/1901</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>81.4</td>\n",
       "      <td>80.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/1901</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>67.8</td>\n",
       "      <td>82.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/1901</td>\n",
       "      <td>15.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>9.7</td>\n",
       "      <td>57.4</td>\n",
       "      <td>42.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>08/2022</td>\n",
       "      <td>20.7</td>\n",
       "      <td>11.9</td>\n",
       "      <td>16.3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>65.3</td>\n",
       "      <td>47.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>09/2022</td>\n",
       "      <td>17.1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>128.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>10/2022</td>\n",
       "      <td>14.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>11.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>77.2</td>\n",
       "      <td>164.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>11/2022</td>\n",
       "      <td>10.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>84.8</td>\n",
       "      <td>181.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>12/2022</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7.3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>143.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1464 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_year  maxTemp  minTemp  meanTemp  vapPress  cldCover  precipitation\n",
       "0       01/1901      5.9      1.0       3.4       7.1      86.0           79.8\n",
       "1       02/1901      4.6     -0.9       1.8       6.5      81.2           59.8\n",
       "2       03/1901      6.4      0.5       3.5       7.0      81.4           80.6\n",
       "3       04/1901     11.4      3.0       7.2       8.4      67.8           82.8\n",
       "4       05/1901     15.5      5.4      10.4       9.7      57.4           42.1\n",
       "...         ...      ...      ...       ...       ...       ...            ...\n",
       "1459    08/2022     20.7     11.9      16.3      14.2      65.3           47.7\n",
       "1460    09/2022     17.1      9.8      13.4      12.9      80.0          128.5\n",
       "1461    10/2022     14.6      8.8      11.7      12.0      77.2          164.3\n",
       "1462    11/2022     10.9      6.1       8.5      10.1      84.8          181.1\n",
       "1463    12/2022      6.1      1.2       3.7       7.3      85.3          143.6\n",
       "\n",
       "[1464 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9ab5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1464 entries, 0 to 1463\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   month_year     1464 non-null   object \n",
      " 1   maxTemp        1464 non-null   float64\n",
      " 2   minTemp        1464 non-null   float64\n",
      " 3   meanTemp       1464 non-null   float64\n",
      " 4   vapPress       1464 non-null   float64\n",
      " 5   cldCover       1464 non-null   float64\n",
      " 6   precipitation  1464 non-null   float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 80.2+ KB\n"
     ]
    }
   ],
   "source": [
    "climVar1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e683c82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_year</th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minTemp</th>\n",
       "      <th>meanTemp</th>\n",
       "      <th>vapPress</th>\n",
       "      <th>cldCover</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1901-07-01</td>\n",
       "      <td>20.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>70.1</td>\n",
       "      <td>67.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1911-07-01</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>15.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>66.8</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1911-08-01</td>\n",
       "      <td>20.3</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.9</td>\n",
       "      <td>14.6</td>\n",
       "      <td>67.8</td>\n",
       "      <td>68.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1921-07-01</td>\n",
       "      <td>20.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>13.7</td>\n",
       "      <td>66.5</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1934-07-01</td>\n",
       "      <td>20.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70.9</td>\n",
       "      <td>71.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1947-08-01</td>\n",
       "      <td>21.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16.9</td>\n",
       "      <td>15.3</td>\n",
       "      <td>64.5</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>1955-07-01</td>\n",
       "      <td>20.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>15.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>65.8</td>\n",
       "      <td>31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>1955-08-01</td>\n",
       "      <td>20.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>16.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>67.3</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>1975-08-01</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>68.5</td>\n",
       "      <td>67.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>1976-07-01</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>16.3</td>\n",
       "      <td>14.3</td>\n",
       "      <td>67.9</td>\n",
       "      <td>48.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>1976-08-01</td>\n",
       "      <td>20.8</td>\n",
       "      <td>10.7</td>\n",
       "      <td>15.7</td>\n",
       "      <td>13.8</td>\n",
       "      <td>58.7</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>1983-07-01</td>\n",
       "      <td>21.5</td>\n",
       "      <td>12.1</td>\n",
       "      <td>16.8</td>\n",
       "      <td>15.5</td>\n",
       "      <td>64.9</td>\n",
       "      <td>36.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>1983-08-01</td>\n",
       "      <td>20.3</td>\n",
       "      <td>11.4</td>\n",
       "      <td>15.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>66.3</td>\n",
       "      <td>45.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1984-08-01</td>\n",
       "      <td>20.3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>14.7</td>\n",
       "      <td>67.8</td>\n",
       "      <td>57.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>1989-07-01</td>\n",
       "      <td>20.9</td>\n",
       "      <td>11.5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>63.1</td>\n",
       "      <td>45.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>1990-08-01</td>\n",
       "      <td>20.1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>15.9</td>\n",
       "      <td>14.5</td>\n",
       "      <td>72.6</td>\n",
       "      <td>77.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>1995-07-01</td>\n",
       "      <td>20.8</td>\n",
       "      <td>11.8</td>\n",
       "      <td>16.3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>68.4</td>\n",
       "      <td>62.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1995-08-01</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>53.6</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1997-08-01</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>15.7</td>\n",
       "      <td>69.4</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>2003-08-01</td>\n",
       "      <td>20.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>67.1</td>\n",
       "      <td>30.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>21.8</td>\n",
       "      <td>12.3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>61.3</td>\n",
       "      <td>54.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>21.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>14.9</td>\n",
       "      <td>63.9</td>\n",
       "      <td>70.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>20.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>16.2</td>\n",
       "      <td>14.7</td>\n",
       "      <td>67.3</td>\n",
       "      <td>68.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>21.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>65.5</td>\n",
       "      <td>63.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>20.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>73.5</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>20.2</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16.2</td>\n",
       "      <td>15.3</td>\n",
       "      <td>74.7</td>\n",
       "      <td>73.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>20.4</td>\n",
       "      <td>11.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>13.9</td>\n",
       "      <td>74.3</td>\n",
       "      <td>43.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>20.7</td>\n",
       "      <td>11.9</td>\n",
       "      <td>16.3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>65.3</td>\n",
       "      <td>47.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_year  maxTemp  minTemp  meanTemp  vapPress  cldCover  precipitation\n",
       "6    1901-07-01     20.5     11.5      16.0      14.1      70.1           67.9\n",
       "126  1911-07-01     20.5     10.9      15.7      13.9      66.8           54.0\n",
       "127  1911-08-01     20.3     11.5      15.9      14.6      67.8           68.9\n",
       "246  1921-07-01     20.6     11.0      15.8      13.7      66.5           65.8\n",
       "402  1934-07-01     20.5     11.8      16.1      15.0      70.9           71.7\n",
       "559  1947-08-01     21.6     12.2      16.9      15.3      64.5           12.8\n",
       "654  1955-07-01     20.8     10.8      15.8      14.0      65.8           31.5\n",
       "655  1955-08-01     20.7     11.7      16.2      14.8      67.3           34.7\n",
       "895  1975-08-01     21.0     12.0      16.5      15.6      68.5           67.3\n",
       "906  1976-07-01     21.0     11.6      16.3      14.3      67.9           48.1\n",
       "907  1976-08-01     20.8     10.7      15.7      13.8      58.7           23.5\n",
       "990  1983-07-01     21.5     12.1      16.8      15.5      64.9           36.9\n",
       "991  1983-08-01     20.3     11.4      15.8      14.6      66.3           45.7\n",
       "1003 1984-08-01     20.3     11.2      15.7      14.7      67.8           57.3\n",
       "1062 1989-07-01     20.9     11.5      16.2      14.4      63.1           45.4\n",
       "1075 1990-08-01     20.1     11.9      15.9      14.5      72.6           77.5\n",
       "1134 1995-07-01     20.8     11.8      16.3      14.8      68.4           62.5\n",
       "1135 1995-08-01     22.0     12.0      17.0      14.8      53.6           19.6\n",
       "1159 1997-08-01     21.0     12.9      16.9      15.7      69.4           86.4\n",
       "1231 2003-08-01     20.5     12.0      16.2      14.5      67.1           30.5\n",
       "1266 2006-07-01     21.8     12.3      17.0      15.1      61.3           54.1\n",
       "1350 2013-07-01     21.1     12.0      16.5      14.9      63.9           70.3\n",
       "1362 2014-07-01     20.5     11.9      16.2      14.7      67.3           68.2\n",
       "1410 2018-07-01     21.2     12.0      16.6      14.5      65.5           63.8\n",
       "1422 2019-07-01     20.1     12.0      16.0      14.9      73.5           86.0\n",
       "1446 2021-07-01     20.2     12.2      16.2      15.3      74.7           73.2\n",
       "1458 2022-07-01     20.4     11.8      16.1      13.9      74.3           43.3\n",
       "1459 2022-08-01     20.7     11.9      16.3      14.2      65.3           47.7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1[climVar1['maxTemp']>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08dbf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "climVar1['month_year'] = pd.to_datetime(climVar1['month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80c9b8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "868c8360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1464 entries, 0 to 1463\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   month_year     1464 non-null   datetime64[ns]\n",
      " 1   maxTemp        1464 non-null   float64       \n",
      " 2   minTemp        1464 non-null   float64       \n",
      " 3   meanTemp       1464 non-null   float64       \n",
      " 4   vapPress       1464 non-null   float64       \n",
      " 5   cldCover       1464 non-null   float64       \n",
      " 6   precipitation  1464 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(6)\n",
      "memory usage: 80.2 KB\n"
     ]
    }
   ],
   "source": [
    "climVar1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3116b3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month_year       0\n",
       "maxTemp          0\n",
       "minTemp          0\n",
       "meanTemp         0\n",
       "vapPress         0\n",
       "cldCover         0\n",
       "precipitation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fac72f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a box plot for each variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "climVar1.boxplot()\n",
    "plt.title('Box Plot of Features')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c100f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.duplicated of      month_year  maxTemp  minTemp  meanTemp  vapPress  cldCover  precipitation\n",
       "0    1901-01-01      5.9      1.0       3.4       7.1      86.0           79.8\n",
       "1    1901-02-01      4.6     -0.9       1.8       6.5      81.2           59.8\n",
       "2    1901-03-01      6.4      0.5       3.5       7.0      81.4           80.6\n",
       "3    1901-04-01     11.4      3.0       7.2       8.4      67.8           82.8\n",
       "4    1901-05-01     15.5      5.4      10.4       9.7      57.4           42.1\n",
       "...         ...      ...      ...       ...       ...       ...            ...\n",
       "1459 2022-08-01     20.7     11.9      16.3      14.2      65.3           47.7\n",
       "1460 2022-09-01     17.1      9.8      13.4      12.9      80.0          128.5\n",
       "1461 2022-10-01     14.6      8.8      11.7      12.0      77.2          164.3\n",
       "1462 2022-11-01     10.9      6.1       8.5      10.1      84.8          181.1\n",
       "1463 2022-12-01      6.1      1.2       3.7       7.3      85.3          143.6\n",
       "\n",
       "[1464 rows x 7 columns]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1.duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1762a760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month_year       datetime64[ns]\n",
      "maxTemp                 float64\n",
      "minTemp                 float64\n",
      "meanTemp                float64\n",
      "vapPress                float64\n",
      "cldCover                float64\n",
      "precipitation           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataTypes = climVar1.dtypes\n",
    "print(dataTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfd5d27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minTemp</th>\n",
       "      <th>meanTemp</th>\n",
       "      <th>vapPress</th>\n",
       "      <th>cldCover</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1464.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.057377</td>\n",
       "      <td>5.390369</td>\n",
       "      <td>8.699112</td>\n",
       "      <td>9.919604</td>\n",
       "      <td>78.112637</td>\n",
       "      <td>97.796995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.623501</td>\n",
       "      <td>3.686655</td>\n",
       "      <td>4.126692</td>\n",
       "      <td>2.546742</td>\n",
       "      <td>6.041825</td>\n",
       "      <td>39.658748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.400000</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>53.600000</td>\n",
       "      <td>11.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>74.300000</td>\n",
       "      <td>68.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>78.300000</td>\n",
       "      <td>92.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.400000</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>122.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>238.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           maxTemp      minTemp     meanTemp     vapPress     cldCover  \\\n",
       "count  1464.000000  1464.000000  1464.000000  1464.000000  1464.000000   \n",
       "mean     12.057377     5.390369     8.699112     9.919604    78.112637   \n",
       "std       4.623501     3.686655     4.126692     2.546742     6.041825   \n",
       "min       1.400000    -3.600000    -1.100000     4.900000    53.600000   \n",
       "25%       8.000000     2.300000     5.200000     7.700000    74.300000   \n",
       "50%      11.900000     5.000000     8.400000     9.300000    78.300000   \n",
       "75%      16.400000     8.900000    12.600000    12.200000    82.500000   \n",
       "max      22.000000    12.900000    17.000000    15.700000    93.000000   \n",
       "\n",
       "       precipitation  \n",
       "count    1464.000000  \n",
       "mean       97.796995  \n",
       "std        39.658748  \n",
       "min        11.100000  \n",
       "25%        68.575000  \n",
       "50%        92.950000  \n",
       "75%       122.200000  \n",
       "max       238.100000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b410af77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_to_plot = 'maxTemp'  \n",
    "\n",
    "# Plot a histogram\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(climVar1[column_to_plot], bins=20, edgecolor='black')  \n",
    "plt.xlabel('month_year')\n",
    "plt.ylabel(column_to_plot)\n",
    "plt.title(f'Histogram of {column_to_plot}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e15466cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_to_plot = 'maxTemp'  # Replace with the column name you want to plot\n",
    "\n",
    "# Count the occurrences of each unique value in the column\n",
    "value_counts = climVar1[column_to_plot].value_counts()\n",
    "\n",
    "# Plot a column chart\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.bar(value_counts.index, value_counts.values, color='blue')\n",
    "plt.xlabel(column_to_plot)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Column Chart of {column_to_plot}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a406df07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minTemp</th>\n",
       "      <th>meanTemp</th>\n",
       "      <th>vapPress</th>\n",
       "      <th>cldCover</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>maxTemp</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975286</td>\n",
       "      <td>0.995108</td>\n",
       "      <td>0.964789</td>\n",
       "      <td>-0.659544</td>\n",
       "      <td>-0.234138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minTemp</th>\n",
       "      <td>0.975286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>0.989920</td>\n",
       "      <td>-0.495477</td>\n",
       "      <td>-0.107202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanTemp</th>\n",
       "      <td>0.995108</td>\n",
       "      <td>0.992259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981961</td>\n",
       "      <td>-0.590524</td>\n",
       "      <td>-0.178921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vapPress</th>\n",
       "      <td>0.964789</td>\n",
       "      <td>0.989920</td>\n",
       "      <td>0.981961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.482615</td>\n",
       "      <td>-0.111899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cldCover</th>\n",
       "      <td>-0.659544</td>\n",
       "      <td>-0.495477</td>\n",
       "      <td>-0.590524</td>\n",
       "      <td>-0.482615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipitation</th>\n",
       "      <td>-0.234138</td>\n",
       "      <td>-0.107202</td>\n",
       "      <td>-0.178921</td>\n",
       "      <td>-0.111899</td>\n",
       "      <td>0.540236</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                maxTemp   minTemp  meanTemp  vapPress  cldCover  precipitation\n",
       "maxTemp        1.000000  0.975286  0.995108  0.964789 -0.659544      -0.234138\n",
       "minTemp        0.975286  1.000000  0.992259  0.989920 -0.495477      -0.107202\n",
       "meanTemp       0.995108  0.992259  1.000000  0.981961 -0.590524      -0.178921\n",
       "vapPress       0.964789  0.989920  0.981961  1.000000 -0.482615      -0.111899\n",
       "cldCover      -0.659544 -0.495477 -0.590524 -0.482615  1.000000       0.540236\n",
       "precipitation -0.234138 -0.107202 -0.178921 -0.111899  0.540236       1.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climVar1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "568bdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = climVar1[['minTemp', 'meanTemp', 'cldCover', 'vapPress', 'precipitation']]\n",
    "target = climVar1['maxTemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "71edd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f069a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9bec97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1eccc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9b2d50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1796 - val_loss: 0.1244\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 0.1157\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 0.1152\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1585 - val_loss: 0.1078\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1522 - val_loss: 0.1068\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 0.1017\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1436 - val_loss: 0.0995\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1388 - val_loss: 0.0958\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1349 - val_loss: 0.0934\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1310 - val_loss: 0.0900\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1282 - val_loss: 0.0945\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1250 - val_loss: 0.0880\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1222 - val_loss: 0.0850\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1184 - val_loss: 0.0848\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1162 - val_loss: 0.0850\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 0.0789\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 0.0791\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1085 - val_loss: 0.0792\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1073 - val_loss: 0.0778\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1042 - val_loss: 0.0747\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1030 - val_loss: 0.0769\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1003 - val_loss: 0.0715\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0987 - val_loss: 0.0704\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0963 - val_loss: 0.0689\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0940 - val_loss: 0.0722\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0931 - val_loss: 0.0679\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0925 - val_loss: 0.0683\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0903 - val_loss: 0.0673\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0880 - val_loss: 0.0659\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 0.0643\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.0608\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0838 - val_loss: 0.0619\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0831 - val_loss: 0.0629\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0815 - val_loss: 0.0616\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0797 - val_loss: 0.0594\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 0.0599\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0775 - val_loss: 0.0562\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.0563\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0750 - val_loss: 0.0553\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0752 - val_loss: 0.0547\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0726 - val_loss: 0.0534\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0549\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0513\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0683 - val_loss: 0.0506\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.0506\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0661 - val_loss: 0.0502\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0656 - val_loss: 0.0495\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0636 - val_loss: 0.0512\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0641 - val_loss: 0.0501\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0616 - val_loss: 0.0498\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0498\n",
      "Test Loss_10,10,0.001,all: 0.04983823001384735\n"
     ]
    }
   ],
   "source": [
    "# Loading the data into DataFrame\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss_10,10,0.001,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4100be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae6547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 141.3283 - val_loss: 134.4599\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 105.1212 - val_loss: 84.3687\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 52.3461 - val_loss: 29.2770\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17.6935 - val_loss: 12.3445\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.5576 - val_loss: 10.3655\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.9286 - val_loss: 9.0034\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.7499 - val_loss: 7.8735\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.7583 - val_loss: 6.9345\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.8924 - val_loss: 6.0787\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.0585 - val_loss: 5.3055\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.2982 - val_loss: 4.5456\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.5781 - val_loss: 3.8725\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.9182 - val_loss: 3.2837\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3107 - val_loss: 2.7551\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.7868 - val_loss: 2.2782\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3215 - val_loss: 1.8524\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8901 - val_loss: 1.4856\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5275 - val_loss: 1.1918\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.2220 - val_loss: 0.9249\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.9739 - val_loss: 0.7376\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7793 - val_loss: 0.5800\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6253 - val_loss: 0.4685\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5074 - val_loss: 0.3763\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4146 - val_loss: 0.3111\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3450 - val_loss: 0.2643\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2925 - val_loss: 0.2238\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 0.1966\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2159 - val_loss: 0.1723\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1897 - val_loss: 0.1564\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 0.1449\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1568 - val_loss: 0.1392\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 0.1290\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1367 - val_loss: 0.1201\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1288 - val_loss: 0.1114\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1231 - val_loss: 0.1074\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 0.1073\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1129 - val_loss: 0.1035\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1095 - val_loss: 0.0992\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1047 - val_loss: 0.0984\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1007 - val_loss: 0.0909\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.0876\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 0.0850\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.0823\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.0837\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 0.0829\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 0.0751\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 0.0740\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0719\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.0703\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 0.0703\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0703\n",
      "Test Loss 20,20,0.001,all: 0.070307157933712\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20,20,0.001,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32edfb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 155.2609 - val_loss: 148.9319\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 119.7764 - val_loss: 101.9046\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 67.7479 - val_loss: 41.2186\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21.7781 - val_loss: 10.4342\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.6521 - val_loss: 8.4931\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.4314 - val_loss: 7.4077\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.4487 - val_loss: 6.4991\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.5227 - val_loss: 5.6199\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.6451 - val_loss: 4.7917\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.8044 - val_loss: 3.9996\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.0225 - val_loss: 3.2927\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2976 - val_loss: 2.6810\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6692 - val_loss: 2.1222\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1106 - val_loss: 1.6610\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6730 - val_loss: 1.2924\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3132 - val_loss: 1.0132\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0357 - val_loss: 0.7959\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8226 - val_loss: 0.6214\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6552 - val_loss: 0.4917\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5233 - val_loss: 0.3962\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4239 - val_loss: 0.3226\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3545 - val_loss: 0.2678\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2971 - val_loss: 0.2264\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2557 - val_loss: 0.2039\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 0.1853\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2019 - val_loss: 0.1640\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 0.1583\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 0.1551\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 0.1456\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1499 - val_loss: 0.1370\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1417 - val_loss: 0.1257\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1351 - val_loss: 0.1312\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 0.1231\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1146\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1172 - val_loss: 0.1152\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 0.1061\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1078 - val_loss: 0.1052\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 0.1046\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 0.0954\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.0928\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 0.0893\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0888 - val_loss: 0.0866\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0856 - val_loss: 0.0857\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0820 - val_loss: 0.0806\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0795 - val_loss: 0.0782\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 0.0731\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 0.0760\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0696\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.0686\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.0664\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Test Loss 30,30,0.001,all: 0.0663544163107872\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 30,30,0.001,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82512a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 163.7744 - val_loss: 169.9908\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 155.8996 - val_loss: 161.7571\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 148.2272 - val_loss: 153.6687\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 140.7590 - val_loss: 145.8232\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 133.5523 - val_loss: 138.4184\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 126.4418 - val_loss: 130.6305\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 118.9867 - val_loss: 122.3986\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 110.6538 - val_loss: 112.6955\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 101.1582 - val_loss: 102.0353\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 91.1905 - val_loss: 91.1038\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 81.2095 - val_loss: 80.5820\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 71.6767 - val_loss: 70.4091\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 62.6409 - val_loss: 61.1643\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 54.3949 - val_loss: 52.5257\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 46.8846 - val_loss: 44.8796\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 40.2572 - val_loss: 38.0907\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 34.3981 - val_loss: 32.4788\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 29.4702 - val_loss: 27.5791\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 25.2750 - val_loss: 23.6277\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21.8290 - val_loss: 20.4235\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19.0418 - val_loss: 17.8614\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16.7882 - val_loss: 15.8360\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14.9622 - val_loss: 14.2304\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13.4925 - val_loss: 12.9103\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12.2337 - val_loss: 11.8128\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 11.1522 - val_loss: 10.8055\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 10.1874 - val_loss: 9.9243\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 9.3461 - val_loss: 9.1920\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 8.6519 - val_loss: 8.5699\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 8.0488 - val_loss: 8.0029\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.4974 - val_loss: 7.4690\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 6.9739 - val_loss: 6.9533\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 6.4849 - val_loss: 6.4639\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 6.0278 - val_loss: 6.0193\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.6164 - val_loss: 5.6109\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.2383 - val_loss: 5.2293\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.8941 - val_loss: 4.8840\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.5912 - val_loss: 4.5755\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.3059 - val_loss: 4.3013\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.0591 - val_loss: 4.0505\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.8243 - val_loss: 3.8129\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.6046 - val_loss: 3.5918\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.4000 - val_loss: 3.3752\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.1934 - val_loss: 3.1777\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.0004 - val_loss: 2.9740\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.8136 - val_loss: 2.7819\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6325 - val_loss: 2.5970\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4565 - val_loss: 2.4212\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2935 - val_loss: 2.2504\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.1337 - val_loss: 2.0886\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 2.0886\n",
      "Test Loss10,0.001,all: 2.0885636806488037\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss10,0.001,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc8db263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 168.0866 - val_loss: 168.8928\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 152.8688 - val_loss: 152.7590\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 137.3840 - val_loss: 135.8702\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 121.3293 - val_loss: 118.3050\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 104.9989 - val_loss: 101.1638\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 89.0233 - val_loss: 84.2927\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 73.7912 - val_loss: 68.4667\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 59.8984 - val_loss: 54.6728\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 47.8456 - val_loss: 42.9911\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 37.8753 - val_loss: 33.6528\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 30.1153 - val_loss: 26.8041\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 24.5028 - val_loss: 21.9960\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 20.6416 - val_loss: 18.7768\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18.0439 - val_loss: 16.6585\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 16.2769 - val_loss: 15.2182\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 15.0029 - val_loss: 14.0981\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 13.9807 - val_loss: 13.1864\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 13.0854 - val_loss: 12.3733\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12.2858 - val_loss: 11.5978\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 11.4977 - val_loss: 10.8432\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 10.7507 - val_loss: 10.1417\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.0359 - val_loss: 9.4457\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.3489 - val_loss: 8.7889\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.6941 - val_loss: 8.1398\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 8.0493 - val_loss: 7.5141\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 7.4464 - val_loss: 6.9211\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.8828 - val_loss: 6.3704\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.3502 - val_loss: 5.8546\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.8595 - val_loss: 5.3685\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.3952 - val_loss: 4.9352\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.9640 - val_loss: 4.5140\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.5585 - val_loss: 4.1028\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.1673 - val_loss: 3.7334\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8086 - val_loss: 3.3908\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4829 - val_loss: 3.0755\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1743 - val_loss: 2.7864\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.8996 - val_loss: 2.5291\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.6497 - val_loss: 2.2917\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.4195 - val_loss: 2.0847\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.2130 - val_loss: 1.8854\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.0197 - val_loss: 1.7105\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.8415 - val_loss: 1.5478\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.6831 - val_loss: 1.3991\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5356 - val_loss: 1.2664\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4006 - val_loss: 1.1472\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2800 - val_loss: 1.0345\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.1661 - val_loss: 0.9324\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.0642 - val_loss: 0.8416\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9729 - val_loss: 0.7614\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8887 - val_loss: 0.6893\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6893\n",
      "Test Loss 20, 0.001, all: 0.6893036961555481\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20, 0.001, all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bed32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d32c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 155.5034 - val_loss: 156.9739\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 139.3311 - val_loss: 138.7308\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 121.0452 - val_loss: 118.2375\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 101.2953 - val_loss: 96.0693\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 80.6719 - val_loss: 74.4447\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 60.9962 - val_loss: 54.2913\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 43.9162 - val_loss: 37.3328\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 30.2873 - val_loss: 25.4624\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 21.2048 - val_loss: 18.0647\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15.9855 - val_loss: 14.2570\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13.2816 - val_loss: 12.3944\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.8451 - val_loss: 11.2821\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.8788 - val_loss: 10.4515\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.1123 - val_loss: 9.7250\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.4233 - val_loss: 9.0490\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.8077 - val_loss: 8.4027\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 8.2257 - val_loss: 7.8084\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 7.6921 - val_loss: 7.2677\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 7.1939 - val_loss: 6.7767\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 6.7201 - val_loss: 6.2976\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 6.2673 - val_loss: 5.8407\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.8295 - val_loss: 5.4138\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.4174 - val_loss: 4.9848\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.0065 - val_loss: 4.5624\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.6076 - val_loss: 4.1837\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.2381 - val_loss: 3.8195\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.8959 - val_loss: 3.4805\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.5805 - val_loss: 3.1699\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 3.2881 - val_loss: 2.8812\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.0152 - val_loss: 2.6236\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.7646 - val_loss: 2.3826\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.5213 - val_loss: 2.1566\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.2985 - val_loss: 1.9327\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0890 - val_loss: 1.7360\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8952 - val_loss: 1.5586\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7142 - val_loss: 1.3911\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.5401 - val_loss: 1.2462\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3892 - val_loss: 1.1045\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.2464 - val_loss: 0.9799\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.1156 - val_loss: 0.8679\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9985 - val_loss: 0.7706\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8921 - val_loss: 0.6776\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7989 - val_loss: 0.5993\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7151 - val_loss: 0.5323\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6439 - val_loss: 0.4747\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5787 - val_loss: 0.4242\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5212 - val_loss: 0.3805\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4736 - val_loss: 0.3417\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4303 - val_loss: 0.3097\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3932 - val_loss: 0.2842\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2842\n",
      "Test Loss 30, 0.001, all: 0.28419509530067444\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 30, 0.001, all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2a8efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 140.7440 - val_loss: 139.0216\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 121.8869 - val_loss: 117.7570\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 101.0410 - val_loss: 94.1893\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 78.6131 - val_loss: 70.0881\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 57.1457 - val_loss: 48.6864\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 39.4242 - val_loss: 32.4836\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 26.9924 - val_loss: 22.2191\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19.6854 - val_loss: 16.9920\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16.0270 - val_loss: 14.4478\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14.0553 - val_loss: 13.0207\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.7096 - val_loss: 11.9383\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.6150 - val_loss: 10.9077\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 10.6158 - val_loss: 9.9780\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.7081 - val_loss: 9.0635\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.8532 - val_loss: 8.2488\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.0554 - val_loss: 7.4601\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.3027 - val_loss: 6.7164\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.6109 - val_loss: 6.0398\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.9699 - val_loss: 5.3901\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.3620 - val_loss: 4.8183\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.8066 - val_loss: 4.2685\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.2882 - val_loss: 3.7690\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.8120 - val_loss: 3.3062\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3760 - val_loss: 2.8765\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9754 - val_loss: 2.4930\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.5976 - val_loss: 2.1652\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.2788 - val_loss: 1.8715\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9880 - val_loss: 1.6143\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.7299 - val_loss: 1.3872\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.5051 - val_loss: 1.1888\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3155 - val_loss: 1.0205\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.1467 - val_loss: 0.8820\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.0046 - val_loss: 0.7678\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.8844 - val_loss: 0.6637\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7784 - val_loss: 0.5799\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6883 - val_loss: 0.5171\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6147 - val_loss: 0.4545\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5485 - val_loss: 0.4074\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4966 - val_loss: 0.3671\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4520 - val_loss: 0.3359\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4136 - val_loss: 0.3066\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3812 - val_loss: 0.2877\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3538 - val_loss: 0.2668\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3294 - val_loss: 0.2497\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3104 - val_loss: 0.2354\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2902 - val_loss: 0.2261\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2733 - val_loss: 0.2112\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2594 - val_loss: 0.2020\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2472 - val_loss: 0.1919\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2341 - val_loss: 0.1837\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1837\n",
      "Test Loss 40, 0.001, all: 0.18372927606105804\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 40, 0.001, all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27fdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7da6d3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 137.5665 - val_loss: 129.5733\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 112.7526 - val_loss: 101.4472\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 87.2357 - val_loss: 75.1308\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 65.4996 - val_loss: 54.7216\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 50.2269 - val_loss: 42.8205\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 42.0580 - val_loss: 37.2705\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 38.1427 - val_loss: 34.8719\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 35.9858 - val_loss: 33.0896\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 33.9011 - val_loss: 31.1908\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 31.6694 - val_loss: 29.0091\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 29.1343 - val_loss: 26.5031\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 26.2037 - val_loss: 23.5263\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 22.7458 - val_loss: 20.1163\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19.0109 - val_loss: 16.6285\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15.3553 - val_loss: 13.2359\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.0714 - val_loss: 10.2490\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.3565 - val_loss: 7.8478\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.3203 - val_loss: 6.1103\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.9015 - val_loss: 4.8749\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.8967 - val_loss: 4.0396\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.1889 - val_loss: 3.4318\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.6443 - val_loss: 2.9554\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.1922 - val_loss: 2.5676\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.7886 - val_loss: 2.2376\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.4431 - val_loss: 1.9529\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.1376 - val_loss: 1.7102\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.8819 - val_loss: 1.5007\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.6550 - val_loss: 1.3274\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.4673 - val_loss: 1.1736\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3024 - val_loss: 1.0417\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1654 - val_loss: 0.9326\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0509 - val_loss: 0.8386\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.9551 - val_loss: 0.7587\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.8703 - val_loss: 0.6844\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7951 - val_loss: 0.6232\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7318 - val_loss: 0.5713\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6744 - val_loss: 0.5244\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6228 - val_loss: 0.4845\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5784 - val_loss: 0.4482\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5394 - val_loss: 0.4155\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5028 - val_loss: 0.3920\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4721 - val_loss: 0.3630\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4425 - val_loss: 0.3421\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4159 - val_loss: 0.3225\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3927 - val_loss: 0.3042\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3702 - val_loss: 0.2899\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3515 - val_loss: 0.2725\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3324 - val_loss: 0.2592\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3166 - val_loss: 0.2462\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3004 - val_loss: 0.2351\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2351\n",
      "Test Loss 10,0.002,all: 0.23514139652252197\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 10,0.002,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316692ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4ea2622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 155.5973 - val_loss: 153.7454\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 132.0057 - val_loss: 126.3555\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 103.4946 - val_loss: 92.9905\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 71.6987 - val_loss: 59.1949\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 43.5392 - val_loss: 32.9883\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 24.7480 - val_loss: 18.6105\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15.7195 - val_loss: 13.1586\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.2805 - val_loss: 11.2167\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.6446 - val_loss: 9.9086\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.3971 - val_loss: 8.7682\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.3567 - val_loss: 7.7742\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.4631 - val_loss: 6.8882\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.6533 - val_loss: 6.0605\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.9002 - val_loss: 5.2944\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.1878 - val_loss: 4.5702\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.5101 - val_loss: 3.8851\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8612 - val_loss: 3.2793\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2956 - val_loss: 2.7702\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8009 - val_loss: 2.3369\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3798 - val_loss: 1.9556\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0196 - val_loss: 1.6122\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7041 - val_loss: 1.3368\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4283 - val_loss: 1.0904\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1978 - val_loss: 0.8987\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9998 - val_loss: 0.7247\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.8388 - val_loss: 0.5910\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7035 - val_loss: 0.4807\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5911 - val_loss: 0.4014\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5034 - val_loss: 0.3356\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4301 - val_loss: 0.2838\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3707 - val_loss: 0.2398\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3243 - val_loss: 0.2095\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2826 - val_loss: 0.1839\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 0.1615\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2235 - val_loss: 0.1473\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2037 - val_loss: 0.1325\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1861 - val_loss: 0.1219\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 0.1142\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1602 - val_loss: 0.1076\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1510 - val_loss: 0.1046\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1431 - val_loss: 0.0992\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1384 - val_loss: 0.0974\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 0.0919\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1272 - val_loss: 0.0968\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 0.0910\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1203 - val_loss: 0.0877\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1180 - val_loss: 0.0895\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1163 - val_loss: 0.0878\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 0.0839\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 0.0836\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0836\n",
      "Test Loss 20, 0.002, all: 0.0836033895611763\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20, 0.002, all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec7988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a35ff3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 172.0036 - val_loss: 165.1904\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 142.0769 - val_loss: 136.8532\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 115.2293 - val_loss: 107.6800\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 86.9943 - val_loss: 76.3117\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 57.9971 - val_loss: 45.5315\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 32.8859 - val_loss: 23.5141\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17.7506 - val_loss: 13.4484\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.7091 - val_loss: 9.9802\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.2861 - val_loss: 8.1766\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.6360 - val_loss: 6.7710\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 6.3803 - val_loss: 5.6903\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.4218 - val_loss: 4.7625\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.6450 - val_loss: 4.0223\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.9745 - val_loss: 3.3911\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3788 - val_loss: 2.8230\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.8488 - val_loss: 2.3041\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.3586 - val_loss: 1.8668\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.9453 - val_loss: 1.4889\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.5782 - val_loss: 1.1664\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.2679 - val_loss: 0.9084\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.0185 - val_loss: 0.7036\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.8186 - val_loss: 0.5604\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6678 - val_loss: 0.4411\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5485 - val_loss: 0.3548\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 0.2983\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3936 - val_loss: 0.2521\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3401 - val_loss: 0.2154\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2978 - val_loss: 0.1938\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2651 - val_loss: 0.1760\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2387 - val_loss: 0.1571\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2165 - val_loss: 0.1439\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1972 - val_loss: 0.1299\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1829 - val_loss: 0.1255\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 0.1160\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1574 - val_loss: 0.1096\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1484 - val_loss: 0.1030\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1411 - val_loss: 0.1034\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1324 - val_loss: 0.0949\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1272 - val_loss: 0.0914\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1219 - val_loss: 0.0870\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1187 - val_loss: 0.0874\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1136 - val_loss: 0.0899\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1105 - val_loss: 0.0860\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1064 - val_loss: 0.0817\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1030 - val_loss: 0.0810\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.0748\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 0.0739\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0963 - val_loss: 0.0746\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0937 - val_loss: 0.0743\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.0699\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0699\n",
      "Test Loss 30, 0.002, all: 0.06993377953767776\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 30, 0.002, all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbef9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8abe0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 142.7347 - val_loss: 129.6484\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 103.8470 - val_loss: 85.5395\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 62.7340 - val_loss: 44.5909\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 32.1048 - val_loss: 22.3847\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 18.4725 - val_loss: 15.4780\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14.3038 - val_loss: 12.7392\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.9993 - val_loss: 10.5647\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.8604 - val_loss: 8.6232\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 8.1064 - val_loss: 6.9772\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.5771 - val_loss: 5.5656\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.2573 - val_loss: 4.3234\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.1103 - val_loss: 3.3207\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1499 - val_loss: 2.4578\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3862 - val_loss: 1.8156\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7759 - val_loss: 1.3311\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3360 - val_loss: 0.9906\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.0231 - val_loss: 0.7478\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7991 - val_loss: 0.5852\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6454 - val_loss: 0.4808\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5340 - val_loss: 0.4090\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4549 - val_loss: 0.3535\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3975 - val_loss: 0.3103\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3535 - val_loss: 0.2837\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3158 - val_loss: 0.2547\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2883 - val_loss: 0.2311\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2619 - val_loss: 0.2109\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 0.1974\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 0.1770\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2009 - val_loss: 0.1629\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1861 - val_loss: 0.1480\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1714 - val_loss: 0.1385\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1575 - val_loss: 0.1257\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1480 - val_loss: 0.1170\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1379 - val_loss: 0.1086\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1311 - val_loss: 0.1055\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1231 - val_loss: 0.0986\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1180 - val_loss: 0.0907\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1137 - val_loss: 0.0865\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1103 - val_loss: 0.0839\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1044 - val_loss: 0.0860\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 0.0806\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0977 - val_loss: 0.0827\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 0.0750\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0938 - val_loss: 0.0696\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0904 - val_loss: 0.0729\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 0.0657\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 0.0681\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0845 - val_loss: 0.0654\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0835 - val_loss: 0.0646\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0796 - val_loss: 0.0614\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0614\n",
      "Test Loss 40, 0.002, all: 0.06140214949846268\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 40, 0.002, all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d7bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88146002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 170.0959 - val_loss: 172.1256\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 154.1498 - val_loss: 154.0759\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 133.5602 - val_loss: 128.0393\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 104.4140 - val_loss: 92.8665\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 69.3420 - val_loss: 54.5402\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 37.5289 - val_loss: 26.2375\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 19.0272 - val_loss: 13.8787\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12.3700 - val_loss: 10.3337\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 9.8644 - val_loss: 8.7004\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.4811 - val_loss: 7.6301\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.5223 - val_loss: 6.8383\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.8044 - val_loss: 6.1530\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.1743 - val_loss: 5.5711\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.5996 - val_loss: 5.0077\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.0743 - val_loss: 4.4886\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.5737 - val_loss: 4.0075\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.1015 - val_loss: 3.5558\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6483 - val_loss: 3.1219\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2303 - val_loss: 2.7314\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8262 - val_loss: 2.3684\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4619 - val_loss: 2.0394\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1352 - val_loss: 1.7317\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8283 - val_loss: 1.4630\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5518 - val_loss: 1.2147\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3044 - val_loss: 0.9947\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0839 - val_loss: 0.8113\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9011 - val_loss: 0.6516\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7466 - val_loss: 0.5297\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6158 - val_loss: 0.4268\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5090 - val_loss: 0.3500\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4226 - val_loss: 0.2930\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3545 - val_loss: 0.2430\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3044 - val_loss: 0.2096\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2646 - val_loss: 0.1812\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2310 - val_loss: 0.1614\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2063 - val_loss: 0.1448\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 0.1372\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1684 - val_loss: 0.1230\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1548 - val_loss: 0.1149\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1453 - val_loss: 0.1097\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1353 - val_loss: 0.1073\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1293 - val_loss: 0.0997\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 0.0956\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1180 - val_loss: 0.0932\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 0.0928\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 0.0898\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 0.0878\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 0.0870\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 0.0827\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0790\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0790\n",
      "Test Loss 20,10,0.001,all: 0.07900454849004745\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20,10,0.001,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "39d49c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 2s 10ms/step - loss: 150.8138 - val_loss: 142.1324\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 111.1234 - val_loss: 88.2805\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 52.6250 - val_loss: 23.9572\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 14.0994 - val_loss: 10.5696\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 9.7451 - val_loss: 8.5685\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 8.0065 - val_loss: 7.0694\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 6.6760 - val_loss: 5.8172\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.5978 - val_loss: 4.7828\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.6694 - val_loss: 3.9540\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 3.8620 - val_loss: 3.1690\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 3.1502 - val_loss: 2.5641\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.5426 - val_loss: 2.0158\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.0181 - val_loss: 1.5976\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1.6244 - val_loss: 1.2806\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 1.3207 - val_loss: 1.0468\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.0953 - val_loss: 0.8689\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.9192 - val_loss: 0.7243\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.7794 - val_loss: 0.6046\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.6598 - val_loss: 0.5134\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.5640 - val_loss: 0.4413\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4858 - val_loss: 0.3707\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.4213 - val_loss: 0.3240\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3724 - val_loss: 0.2813\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3290 - val_loss: 0.2479\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2950 - val_loss: 0.2267\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2664 - val_loss: 0.1994\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.2433 - val_loss: 0.1855\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.2207 - val_loss: 0.1698\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.2033 - val_loss: 0.1556\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1878 - val_loss: 0.1479\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1745 - val_loss: 0.1324\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1591 - val_loss: 0.1264\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1493 - val_loss: 0.1152\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1398 - val_loss: 0.1095\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1321 - val_loss: 0.1030\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1238 - val_loss: 0.0985\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1185 - val_loss: 0.0908\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1122 - val_loss: 0.0854\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1069 - val_loss: 0.0830\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1029 - val_loss: 0.0790\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0975 - val_loss: 0.0744\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.0931 - val_loss: 0.0748\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0890 - val_loss: 0.0746\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0871 - val_loss: 0.0706\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0824 - val_loss: 0.0626\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0807 - val_loss: 0.0649\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0790 - val_loss: 0.0576\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0747 - val_loss: 0.0588\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0735 - val_loss: 0.0584\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.0702 - val_loss: 0.0515\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0515\n",
      "Test Loss 10,10,0.002,all: 0.05153028666973114\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 10,10,0.002,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69168da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "980b1f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 142.5956 - val_loss: 132.5299\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 103.6989 - val_loss: 78.0180\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 49.4192 - val_loss: 28.2082\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19.4747 - val_loss: 13.4773\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.5385 - val_loss: 10.0982\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.2263 - val_loss: 7.4127\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.8875 - val_loss: 5.5025\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.1789 - val_loss: 4.1241\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.9202 - val_loss: 3.0599\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9269 - val_loss: 2.1847\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0364 - val_loss: 1.4640\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3679 - val_loss: 0.9542\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8907 - val_loss: 0.6086\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6110 - val_loss: 0.4113\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4484 - val_loss: 0.3082\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3562 - val_loss: 0.2563\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3023 - val_loss: 0.2132\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2663 - val_loss: 0.1982\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2436 - val_loss: 0.1711\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2236 - val_loss: 0.1717\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2071 - val_loss: 0.1547\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1961 - val_loss: 0.1408\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1909 - val_loss: 0.1548\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1742 - val_loss: 0.1516\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1746 - val_loss: 0.1225\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1619 - val_loss: 0.1163\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1576 - val_loss: 0.1115\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 0.1208\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 0.1081\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1402 - val_loss: 0.0990\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1377 - val_loss: 0.1015\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1361 - val_loss: 0.0956\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 0.0960\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1242 - val_loss: 0.0921\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1202 - val_loss: 0.0913\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1163 - val_loss: 0.1041\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 0.0808\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1107 - val_loss: 0.0866\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1080 - val_loss: 0.0868\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1082 - val_loss: 0.0843\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 0.0799\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 0.0791\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0940 - val_loss: 0.0706\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0938 - val_loss: 0.0660\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0897 - val_loss: 0.0662\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 0.0661\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 0.0641\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0826 - val_loss: 0.0606\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0805 - val_loss: 0.0607\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0794 - val_loss: 0.0581\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0581\n",
      "Test Loss 20,10,0.002,all: 0.0580691434442997\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20,10,0.002,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfce3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86bbc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 141.0613 - val_loss: 112.4421\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 63.2927 - val_loss: 24.7765\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 16.4363 - val_loss: 13.0529\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.4052 - val_loss: 9.5542\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.5057 - val_loss: 7.1531\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.5741 - val_loss: 5.4840\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.0689 - val_loss: 4.0488\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7181 - val_loss: 2.9060\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5951 - val_loss: 1.8929\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.7627 - val_loss: 1.2494\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.1836 - val_loss: 0.8092\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.8082 - val_loss: 0.5551\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5776 - val_loss: 0.4141\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4457 - val_loss: 0.3183\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3514 - val_loss: 0.2480\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2912 - val_loss: 0.2046\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2463 - val_loss: 0.1758\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2139 - val_loss: 0.1556\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1912 - val_loss: 0.1386\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1702 - val_loss: 0.1258\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1546 - val_loss: 0.1249\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1431 - val_loss: 0.1092\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1350 - val_loss: 0.0998\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1247 - val_loss: 0.0952\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1169 - val_loss: 0.0897\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1105 - val_loss: 0.0838\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1054 - val_loss: 0.0815\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0984 - val_loss: 0.0843\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 0.0721\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.0726\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 0.0702\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0834 - val_loss: 0.0660\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0802 - val_loss: 0.0615\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 0.0592\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0738 - val_loss: 0.0586\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0604\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0532\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.0519\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.0492\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0531\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0600 - val_loss: 0.0476\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.0576 - val_loss: 0.0462\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0553 - val_loss: 0.0443\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0529 - val_loss: 0.0438\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.0432\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.0435\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.0375\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0466 - val_loss: 0.0395\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0452 - val_loss: 0.0387\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0446 - val_loss: 0.0355\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0355\n",
      "Test Loss 20,20,0.002,all: 0.035492077469825745\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20,20,0.002,all: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7a047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78d4fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features_1 = climVar1[['minTemp', 'meanTemp', 'vapPress', ]]\n",
    "target = climVar1['maxTemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cacd8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_1, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1c51dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b079f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 159.6774 - val_loss: 164.5372\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 151.4725 - val_loss: 155.7306\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 143.1606 - val_loss: 146.2844\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 134.1560 - val_loss: 136.2456\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 124.5048 - val_loss: 125.4225\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 114.1714 - val_loss: 114.1429\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 103.3991 - val_loss: 102.2816\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 92.2918 - val_loss: 90.2347\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 81.1492 - val_loss: 78.5451\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 70.4197 - val_loss: 67.5324\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 60.4342 - val_loss: 57.4187\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 51.3889 - val_loss: 48.5071\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 43.5366 - val_loss: 40.7085\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 36.8311 - val_loss: 34.3307\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 31.3281 - val_loss: 29.0922\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 26.8638 - val_loss: 24.9953\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 23.3280 - val_loss: 21.7695\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 20.5701 - val_loss: 19.2565\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 18.3980 - val_loss: 17.4247\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 16.7130 - val_loss: 15.9819\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 15.3849 - val_loss: 14.8185\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 14.2824 - val_loss: 13.8877\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 13.3453 - val_loss: 13.0706\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.5006 - val_loss: 12.3137\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 11.7097 - val_loss: 11.5961\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 10.9441 - val_loss: 10.8544\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.1784 - val_loss: 10.1005\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.4160 - val_loss: 9.3509\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 8.6661 - val_loss: 8.6252\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 7.9644 - val_loss: 7.9313\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.3025 - val_loss: 7.2813\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.6944 - val_loss: 6.6754\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.1215 - val_loss: 6.1110\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.5961 - val_loss: 5.5828\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.1077 - val_loss: 5.1009\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.6618 - val_loss: 4.6449\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.2521 - val_loss: 4.2301\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8754 - val_loss: 3.8478\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5292 - val_loss: 3.4979\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2070 - val_loss: 3.1741\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9083 - val_loss: 2.8662\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6320 - val_loss: 2.5860\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.3783 - val_loss: 2.3277\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.1475 - val_loss: 2.1012\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.9405 - val_loss: 1.8921\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1.7588 - val_loss: 1.7076\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.5958 - val_loss: 1.5463\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.4539 - val_loss: 1.4049\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3304 - val_loss: 1.2815\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2226 - val_loss: 1.1689\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.1689\n",
      "Test Loss 10,0.001,temp_vp: 1.1688812971115112\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 10,0.001,temp_vp: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5fce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ccf1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 152.8117 - val_loss: 150.9559\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 136.3363 - val_loss: 134.3970\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 121.8565 - val_loss: 118.9006\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 107.5719 - val_loss: 103.1476\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 93.1966 - val_loss: 87.9492\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 79.6528 - val_loss: 73.5069\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 67.3848 - val_loss: 61.7781\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 57.5503 - val_loss: 52.5887\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 50.2246 - val_loss: 46.2619\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 45.1499 - val_loss: 41.7535\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 40.0418 - val_loss: 36.1107\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 33.8106 - val_loss: 30.1081\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 27.8989 - val_loss: 24.9631\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 23.0938 - val_loss: 21.0543\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 19.6468 - val_loss: 18.2694\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 17.2962 - val_loss: 16.4360\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15.6759 - val_loss: 15.0948\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 14.4514 - val_loss: 14.0000\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 13.3932 - val_loss: 12.9787\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.3613 - val_loss: 11.9800\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 11.3360 - val_loss: 10.9660\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.2879 - val_loss: 9.9304\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 9.2224 - val_loss: 8.8545\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8.1518 - val_loss: 7.7789\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.0908 - val_loss: 6.7564\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.0942 - val_loss: 5.8108\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5.2123 - val_loss: 4.9687\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.4378 - val_loss: 4.2121\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 3.7636 - val_loss: 3.5752\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2029 - val_loss: 3.0342\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.7544 - val_loss: 2.6249\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.4248 - val_loss: 2.3112\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2.1780 - val_loss: 2.0920\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.0005 - val_loss: 1.9252\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.8707 - val_loss: 1.7988\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.7566 - val_loss: 1.6949\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.6660 - val_loss: 1.5933\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.5775 - val_loss: 1.5135\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4989 - val_loss: 1.4281\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.4245 - val_loss: 1.3586\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3533 - val_loss: 1.2870\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.2921 - val_loss: 1.2237\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.2291 - val_loss: 1.1584\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1710 - val_loss: 1.1083\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1193 - val_loss: 1.0505\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0666 - val_loss: 1.0022\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.0192 - val_loss: 0.9574\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.9719 - val_loss: 0.9092\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9273 - val_loss: 0.8646\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8825 - val_loss: 0.8213\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.8213\n",
      "Test Loss 10,0.002,temp_vp: 0.8212828040122986\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 10,0.002,temp_vp: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e090f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3ef471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 142.0221 - val_loss: 125.2203\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 92.0487 - val_loss: 64.4883\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 33.8073 - val_loss: 15.0220\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.7331 - val_loss: 10.2144\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 7.8504 - val_loss: 5.5657\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8315 - val_loss: 2.2658\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3944 - val_loss: 0.8009\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5353 - val_loss: 0.3771\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3391 - val_loss: 0.3096\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3091 - val_loss: 0.2887\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2874 - val_loss: 0.2711\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2742 - val_loss: 0.2542\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2529 - val_loss: 0.2524\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2408 - val_loss: 0.2209\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2313 - val_loss: 0.2099\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 0.2031\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1993 - val_loss: 0.1814\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1862 - val_loss: 0.1801\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1714 - val_loss: 0.1590\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1610 - val_loss: 0.1524\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1503 - val_loss: 0.1399\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1419 - val_loss: 0.1283\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 0.1223\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1203 - val_loss: 0.1095\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 0.1098\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 0.0994\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 0.0874\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 0.0827\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.0725\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 0.0758\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.0637\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.0564\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.0579\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.0573\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.0420\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0385\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.0354\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.0368\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.0289\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.0266\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0297 - val_loss: 0.0257\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0255 - val_loss: 0.0223\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0204\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0220\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0194\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0178 - val_loss: 0.0169\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.0150\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0152 - val_loss: 0.0151\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0133\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0138 - val_loss: 0.0126\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Test Loss 20,20,0.002,temp_vp: 0.012570314109325409\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20,20,0.002,temp_vp: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d218d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features_2 = climVar1[['minTemp', 'meanTemp', 'cldCover', 'precipitation']]\n",
    "target = climVar1['maxTemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "377de819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_1, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "239e95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e017178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 9ms/step - loss: 161.4810 - val_loss: 168.2348\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 155.5155 - val_loss: 162.5173\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 150.1945 - val_loss: 157.4041\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 145.1380 - val_loss: 152.1943\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 140.0055 - val_loss: 146.9262\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 134.8619 - val_loss: 141.5904\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 129.7112 - val_loss: 136.1275\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 124.2948 - val_loss: 130.0409\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 117.9985 - val_loss: 122.7745\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 110.8115 - val_loss: 114.5794\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 103.1832 - val_loss: 105.9841\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 95.2312 - val_loss: 97.4009\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 87.2590 - val_loss: 88.6563\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 79.2860 - val_loss: 80.1044\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 71.4719 - val_loss: 71.7273\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 63.9021 - val_loss: 63.6125\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 56.6096 - val_loss: 55.9073\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 49.6931 - val_loss: 48.8276\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 43.2349 - val_loss: 42.1239\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 37.1850 - val_loss: 35.7583\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 31.3594 - val_loss: 29.4774\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 25.3318 - val_loss: 23.1592\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 19.9337 - val_loss: 18.0813\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 15.6619 - val_loss: 14.1403\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 12.3404 - val_loss: 11.0791\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 9.7317 - val_loss: 8.7724\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 7.7907 - val_loss: 7.0759\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 6.4126 - val_loss: 5.9993\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 5.4928 - val_loss: 5.2414\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 4.8358 - val_loss: 4.6888\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4.3444 - val_loss: 4.2813\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 3.9808 - val_loss: 3.9536\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.6950 - val_loss: 3.7028\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.4752 - val_loss: 3.4816\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 3.2838 - val_loss: 3.3091\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 3.1178 - val_loss: 3.1385\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.9617 - val_loss: 2.9789\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.8163 - val_loss: 2.8227\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.6588 - val_loss: 2.6698\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.5085 - val_loss: 2.5167\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.3623 - val_loss: 2.3672\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.2178 - val_loss: 2.2161\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.0742 - val_loss: 2.0735\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 1.9352 - val_loss: 1.9289\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 1.8040 - val_loss: 1.7904\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1.6697 - val_loss: 1.6592\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 1.5438 - val_loss: 1.5332\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1.4232 - val_loss: 1.4099\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 1.3085 - val_loss: 1.2946\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 1.1948 - val_loss: 1.1713\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.1713\n",
      "Test Loss 10,0.001,temp_clcv_prec: 1.1713005304336548\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 10,0.001,temp_clcv_prec: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccdafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff74cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 161.7912 - val_loss: 162.6850\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 146.1193 - val_loss: 148.7566\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 133.2680 - val_loss: 136.3785\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 122.0145 - val_loss: 125.1669\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 111.9018 - val_loss: 114.6060\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 101.9644 - val_loss: 103.7879\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 91.6034 - val_loss: 92.2204\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 80.2797 - val_loss: 79.0614\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 64.2836 - val_loss: 57.3786\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 44.1049 - val_loss: 35.2511\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 24.8311 - val_loss: 17.8170\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 12.0977 - val_loss: 8.1462\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 6.2518 - val_loss: 4.8862\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.2491 - val_loss: 3.7197\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3955 - val_loss: 3.0994\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.8459 - val_loss: 2.6399\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4187 - val_loss: 2.2481\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0620 - val_loss: 1.9328\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7666 - val_loss: 1.6626\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5224 - val_loss: 1.4389\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3214 - val_loss: 1.2575\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1577 - val_loss: 1.1005\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0222 - val_loss: 0.9757\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9101 - val_loss: 0.8724\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8235 - val_loss: 0.7931\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7487 - val_loss: 0.7197\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6878 - val_loss: 0.6655\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6399 - val_loss: 0.6196\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6018 - val_loss: 0.5833\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5695 - val_loss: 0.5529\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5443 - val_loss: 0.5264\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5205 - val_loss: 0.5029\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5009 - val_loss: 0.4827\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4826 - val_loss: 0.4631\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4662 - val_loss: 0.4479\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4513 - val_loss: 0.4314\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4381 - val_loss: 0.4169\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4239 - val_loss: 0.4029\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4104 - val_loss: 0.3901\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3976 - val_loss: 0.3769\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3850 - val_loss: 0.3647\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3732 - val_loss: 0.3535\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3613 - val_loss: 0.3403\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3509 - val_loss: 0.3289\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3392 - val_loss: 0.3192\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3284 - val_loss: 0.3072\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3178 - val_loss: 0.2964\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3080 - val_loss: 0.2872\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2983 - val_loss: 0.2779\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 0.2687\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2687\n",
      "Test Loss 10,0.002,temp_clcv_prec: 0.2686733305454254\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 10,0.002,temp_clcv_prec: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a6778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1566dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 161.6599 - val_loss: 156.3076\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 129.7421 - val_loss: 109.9912\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 70.1592 - val_loss: 34.7842\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 22.0873 - val_loss: 16.9558\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15.0679 - val_loss: 12.9077\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 10.9015 - val_loss: 8.7656\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 7.0672 - val_loss: 5.6788\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.4904 - val_loss: 3.3884\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5558 - val_loss: 1.9362\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3695 - val_loss: 0.9559\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6770 - val_loss: 0.4864\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3646 - val_loss: 0.2805\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2417 - val_loss: 0.2117\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1986 - val_loss: 0.1820\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1763 - val_loss: 0.1641\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1594 - val_loss: 0.1455\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 0.1353\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1369 - val_loss: 0.1220\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1238 - val_loss: 0.1083\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1113 - val_loss: 0.0999\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 0.0968\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 0.0880\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0876 - val_loss: 0.0774\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.0681\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0673\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0673 - val_loss: 0.0603\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0520\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0556 - val_loss: 0.0497\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0513 - val_loss: 0.0488\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0490 - val_loss: 0.0475\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.0437\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0390 - val_loss: 0.0347\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.0340\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0338 - val_loss: 0.0306\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0311 - val_loss: 0.0276\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0283 - val_loss: 0.0259\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0260 - val_loss: 0.0261\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0244 - val_loss: 0.0256\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0226 - val_loss: 0.0211\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0208\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0200 - val_loss: 0.0208\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0177 - val_loss: 0.0206\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0182 - val_loss: 0.0219\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0167 - val_loss: 0.0146\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0154 - val_loss: 0.0164\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0149 - val_loss: 0.0137\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0125 - val_loss: 0.0119\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0119 - val_loss: 0.0114\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Test Loss 20,20,0.002,temp_clcv_prec: 0.011435706168413162\n"
     ]
    }
   ],
   "source": [
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss 20,20,0.002,temp_clcv_prec: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24755824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
